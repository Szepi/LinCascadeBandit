%!TEX root = Paper.tex

\section{Cascading Bandits}
\label{sec:cascading bandits}

We propose a learning variant of the cascade model (\cref{sec:setting}) and two computationally-efficient algorithms for solving it (\cref{sec:algorithms}). To simplify exposition, all random variables are written in bold.


\subsection{Setting}
\label{sec:setting}

We refer to our learning problem as a \emph{generalized cascading bandit}. Formally, we represent the problem by a tuple $B = (E, P, K)$, where $E = \set{1, \dots, L}$ is a \emph{ground set} of $L$ items, $P$ is a probability distribution over a unit hypercube $\set{0, 1}^E$, and $K \leq L$ is the number of recommended items. We call the bandit \emph{generalized} because the form of the distribution $P$ has not been specified yet.

Let $(\rnd{w}_t)_{t = 1}^n$ be an i.i.d. sequence of $n$ \emph{weights} drawn from $P$, where $\rnd{w}_t \in \set{0, 1}^E$ and $\rnd{w}_t(e)$ is the preference of the user for item $e$ at time $t$. That is, $\rnd{w}_t(e) = 1$ if and only if item $e$ attracts the user at time $t$. The learning agent interacts with our problem as follows. At time $t$, the agent recommends a list of $K$ items $\rnd{A}_t = (\rnd{a}^t_1, \dots, \rnd{a}^t_K) \in \Pi_K(E)$. The list is computed from the observations of the agent up to time $t$. The user examines the list, from the first item $\rnd{a}^t_1$ to the last $\rnd{a}^t_K$, and clicks on the first attractive item. If the user is not attracted by any item, the user does not click on any item. Then time increases to $t + 1$.

The reward of the agent at time $t$ can be written in several forms. For instance, as $\max_k \rnd{w}_t(\rnd{a}^t_k)$, at least one item in list $\rnd{A}_t$ is attractive; or as $f(\rnd{A}_t, \rnd{w}_t)$, where:
\begin{align*}
  f(A, w) = 1 - \prod_{k = 1}^K (1 - w(a_k))\,,
\end{align*}
$A = (a_1, \dots, a_K) \in \Pi_K(E)$, and $w \in \set{0, 1}^E$. This later algebraic form is particularly useful in our proofs.

The agent at time $t$ receives feedback:
\begin{align*}
  \rnd{C}_t = \argmin \set{1 \leq k \leq K: \rnd{w}_t(\rnd{a}^t_k) = 1}\,,
\end{align*}
where we assume that $\argmin \emptyset = \infty$. The feedback $\rnd{C}_t$ is the click of the user. If $\rnd{C}_t \leq K$, the user clicks on item $\rnd{C}_t$. If $\rnd{C}_t = \infty$, the user does not click on any item. Since the user clicks on the first attractive item in the list, we can determine the observed weights of all recommended items at time $t$ from $\rnd{C}_t$. In particular, note that:
\begin{align}
  \rnd{w}_t(\rnd{a}^t_k) = \I{\rnd{C}_t = k} \quad k = 1, \dots, \min \set{\rnd{C}_t, K}\,.
  \label{eq:click}
\end{align}
We say that item $e$ is \emph{observed} at time $t$ if $e = \rnd{a}^t_k$ for some $1 \leq k \leq \min \set{\rnd{C}_t, K}$.

In the cascade model (\cref{sec:background}), the weights of the items in the ground set $E$ are distributed independently. We also make this assumption.

\begin{assumption}
\label{ass:independence} The weights $w$ are distributed as:
\begin{align*}
  P(w) = \prod_{e \in E} P_e(w(e))\,,
\end{align*}
where $P_e$ is a Bernoulli distribution with mean $\bar{w}(e)$.
\end{assumption}

Under this assumption, we refer to our learning problem as a \emph{cascading bandit}. In this new problem, the weight of any item at time $t$ is drawn independently of the weights of the other items at that, or any other, time. This assumption has profound consequences and leads to a particularly efficient learning algorithm in \cref{sec:algorithms}. More specifically, under our assumption, the expected reward for list $A \in \Pi_K(E)$, the probability that at least one item in $A$ is attractive, can be expressed as $\EE{f(A, \rnd{w})} = f(A, \bar{w})$, and depends only on the attraction probabilities of individual items in $A$.

The agent's policy is evaluated by its \emph{expected cumulative regret}:
\begin{align*}
  R(n) = \EE{\sum_{t = 1}^n R(\rnd{A}_t, \rnd{w}_t)}\,,
\end{align*}
where $R(\rnd{A}_t, \rnd{w}_t) = f(A^\ast, \rnd{w}_t) - f(\rnd{A}_t, \rnd{w}_t)$ is the \emph{instantaneous stochastic regret} of the agent at time $t$ and:
\begin{align*}
  A^\ast = \argmax_{A \in \Pi_K(E)} f(A, \bar{w})
\end{align*}
is the \emph{optimal list} of items, the list that maximized the reward at any time $t$. Since $f$ is invariant to the permutation of $A$, there exist at least $K!$ optimal lists. For simplicity of exposition, we assume that the optimal solution, as a set, is unique.


\subsubsection{Previous work}
\todoc[inline]{Here we should summarize here what is known, the previous algorithms. Include the previous results.}

Recently, \citet{kveton15cascade} introduced two algorithms,  $\cascadeucb$ and $\cascadeklucb$
and proved upper bounds on their $n$-step regret.
\todoc{Describe here briefly what the algorithms do.}
Here, we state the main results of this work.
To state these results, we need some extra definitions.

In particular, without loss of generality, we assume that the items in the ground set $E$
 are sorted in decreasing order of their attraction probabilities, $\bar{w}(1) \geq \ldots \geq \bar{w}(L)$. In this setting, the \emph{optimal solution} is $A^\ast = (1, \dots, K)$, and contains the first $K$ items in $E$. We say that item $e$ is \emph{optimal} if $1 \leq e \leq K$. Similarly, we say that item $e$ is \emph{suboptimal} if $K < e \leq L$. The \emph{gap} between the attraction probabilities of suboptimal item $e$ and optimal item $e^\ast$:
\begin{align}
  \Delta_{e, e^\ast} = \bar{w}(e^\ast) - \bar{w}(e)
  \label{eq:gap}
\end{align}
measures the hardness of discriminating the items. Whenever convenient, we view an ordered list of items as the set of items on that list.

The first main result of  \citet{kveton15cascade} is a bound on the regret of $\cascadeucb$:
\begin{theorem}
\label{thm:ucb1} The expected $n$-step regret of $\cascadeucb$ is bounded as:
\begin{align*}
  R(n) \leq
  \sum_{e = K + 1}^L \frac{12}{\Delta_{e, K}} \log n + \frac{\pi^2}{3} L\,.
\end{align*}
\end{theorem}
\todoc[inline]{Derive a bound on the worst-case regret of $\cascadeucb$ and include it here. How does it scale with $L$?}

\citet{kveton15cascade}  also provided a result, proving a bound on the regret of $\cascadeklucb$, which we include next.
In this result, $\kl{a}{b}$ stands for the binary relative entropy, i.e., the Kullback Leibler (KL) divergence between the Bernoulli random variables with respective parameters $0\le a,b\le 1$.
\begin{theorem}
\label{thm:klucb} For any $\eps > 0$, the expected $n$-step regret of $\cascadeklucb$ is bounded as:
\begin{align*}
  R(n)
  & \leq \sum_{e = K + 1}^L
  \frac{(1 + \eps) \Delta_{e, K} (1 + \log(1 / \Delta_{e, K}))}{\kl{\bar{w}(e)}{\bar{w}(K)}} (\log n + 3 \log \log n) + C\,,
\end{align*}
where $C = K L \frac{C_2(\eps)}{n^{\beta(\eps)}} + 7 K \log \log n$, and the constants $C_2(\eps)$ and $\beta(\eps)$ are defined in \citet{garivier11klucb}. 
\end{theorem}
\todoc[inline]{Derive a bound on the worst-case regret of $\cascadeklucb$ and include it here. How does it scale with $L$?}

\subsection{Linear Cascading Bandits}
As can be seen, the regret for both algorithms scales linearly with $L$, the number of items.
\todoc{Prove it. Prove lower bound!}
Oftentimes, the items come with features which may allow one to generalize across different items.
In such a case we may hope to eliminate the dependence of the regret on $L$.
This motivates our next assumption:
\begin{assumption}[Linear mean weights]
\label{ass:linear} 
There exists a feature map, $\phi:E \to \Real^d$, known to the learner, such that 
the mean weight $\bar{w}(e)$ satisfies
\begin{align*}
  \bar{w}(e) = \ip{\theta, \phi(e)}\,
\end{align*}
for all items $e\in E$, where $\theta_*\in \R^d$ is an unknown parameter vector
and $\ip{\cdot,\cdot}$ denotes the inner product of its arguments. \todoc{Perhaps this should be moved to elsewhere.}
\end{assumption}
A Cascading Bandit Problem that also satisfies \cref{ass:linear} is called a ``Linear Cascading Bandit'' problem.

\subsection{Algorithms}
\label{sec:algorithms}

We propose two algorithms for solving cascading bandits, $\lincascadeucb$, building on $\cascadeucb$ of 
  \citet{kveton15cascade}, which itself builds on $\ucb$ of \cite{auer02finitetime}.

\cref{alg:ucb} gives the pseudocode of $\lincascadeucb$. 
The algorithm is identical to $\cascadeucb$ except that it estimates the 
\emph{upper confidence bound (UCB)} $\rnd{U}_t(e)$ on the attraction probability of item $e$ at time $t$
in a way that exploits \cref{ass:linear}. In any case, given the estimates $\rnd{U}_t(e)$,
$\lincascadeucb$ recommends a list of $K$ items with largest UCBs:
\begin{align}
  \rnd{A}_t = \argmax_{A \in \Pi_K(E)} f(A, \rnd{U}_t)\,.
\end{align}
Note that $\rnd{A}_t$ is determined only up to a permutation of the items in it. The payoff is not affected by this ordering. But the observations are. For now, we leave the order of items unspecified and return to it later in our discussions. After the user provides feedback $\rnd{C}_t$, the algorithms update the estimate of $\theta_*$ 
based on \eqref{eq:click}, for all $e = \rnd{a}^t_k$ where $k \leq \rnd{C}_t$.
In particular, the estimate computed is
\begin{align}
\label{eq:ridge}
\hat{\theta}_t = \argmin_{\theta\in \R^d}
\lambda \norm{\theta}^2 + 
 \sum_{s=1}^t \sum_{k=1}^{\min(K,\rnd{C}_t)} ( \I{\rnd{C}_t = k} - \ip{\theta,\phi(\rnd{a}^t_k)})^2 \,,
\end{align}
where $\lambda>0$ is a tuning parameter and $\norm{\cdot}$ denotes the $2$-norm.
Note that $\hat{\theta}_t$ can be updated incrementally. \todoc{Add details. Other computational considerations can be discussed later, i.e., $\hat{\theta}_t$ does need to be recomputed too frequently. See Yasin's thesis.}

\begin{algorithm}[t]
  \caption{UCB-like algorithm for cascading bandits.}
  \label{alg:ucb}
  \begin{algorithmic}
    \STATE // Initialization
    \STATE $\hat{\theta}_t \gets 0$
    \FORALL{$t = 1, \dots, n$}
      \STATE Compute UCBs $\rnd{U}_t(e)$ (\cref{sec:algorithms})
      \STATE // Recommend a list of $K$ items and get feedback
      \STATE Let $\rnd{a}^t_1, \dots, \rnd{a}^t_K$ be $K$ items with largest UCBs
      \STATE $\rnd{A}_t \gets (\rnd{a}^t_1, \dots, \rnd{a}^t_K)$
      \STATE Observe click $\rnd{C}_t \in \set{1, \dots, K, \infty}$
      \STATE // Update statistics
      \STATE Update $\hat{\rnd{\theta}}_t$ using~\eqref{eq:ridge}.
    \ENDFOR
  \end{algorithmic}
\end{algorithm}

The UCBs are computed as follows. In $\cascadeucb$, the UCB on the attraction probability of item $e$ at time $t$ is:
\begin{align*}
  \rnd{U}_t(e) = \ip{\hat{\rnd{\theta}}_t,\phi(e)} + \dots\,.
\end{align*}
\todoc{Compute confidence interval that contains the true value with probability $1-\delta$ based on Yasin's thesis.
}

