%!TEX root = Paper.tex

\section{Linear Cascading Bandits}
\label{sec:cascading bandits}

For clarity of the exposition, all random variables are written in bold.


\subsection{Setting}
\label{sec:setting}

We refer to our learning problem as a \emph{linear cascading bandit}.
A problem instance of linear cascading bandits is given
by a tuple $B = (E, \psi, \theta, K)$,
where $E = \set{1, \dots, L}$ is a \emph{ground set} of $L$ items,
$\psi : E \rightarrow \mathbb{R}^d$ is a mapping assigning features to each items,
$\theta \in \mathbb{R}^d$ is an unknown parameter vector,
and $K > 0$ is an integer representing the number of items to be recommended.
The features and the parameter vector define the mean attractiveness of the items, which must lie in the $[0,1]$ interval.
For brevity, we also denote $\psi(e)^\top\theta$ by $\bar{w}(e)$.

The interaction between the learning agent and the environment is as follows:
At time $t$, a user arrives at the web page. The user's preferences for the
items is completely capture by the random binary vector $\rnd{w}_t\in \set{0,1}^E$, whose $e$th component is drawn from $\mathrm{Ber}( \bar{w}(e))$ independently of the other components and previous draws.
The learning agent then recommends a list of $K$ items
$\rnd{A}_t = (\rnd{a}^t_1, \dots, \rnd{a}^t_K) \in \Pi_K(E)$.
The list is computed from the observations of the agent up to time $t$.
The user examines the list,
  from the first item $\rnd{a}^t_1$ to the last $\rnd{a}^t_K$,
  and clicks on the first attractive item.
If the user is not attracted by any item,
  the user does not click on any item. Then time increases to $t + 1$.

The reward of the agent at time $t$ can be written in several forms.
For instance, as $\max_k \rnd{w}_t(\rnd{a}^t_k)$, at least one item in list $\rnd{A}_t$ is attractive; or as $f(\rnd{A}_t, \rnd{w}_t)$, where:
\begin{align*}
  f(A, w) = 1 - \prod_{k = 1}^K (1 - w(a_k))\,,
\end{align*}
$A = (a_1, \dots, a_K) \in \Pi_K(E)$, and $w \in \set{0, 1}^E$. This later algebraic form is particularly useful in our proofs.

The agent at time $t$ receives feedback:
\begin{align*}
  \rnd{C}_t = \argmin \set{1 \leq k \leq K: \rnd{w}_t(\rnd{a}^t_k) = 1}\,,
\end{align*}
where we assume that $\argmin \emptyset = \infty$. The feedback $\rnd{C}_t$ is the click of the user. If $\rnd{C}_t \leq K$, the user clicks on item $\rnd{C}_t$. If $\rnd{C}_t = \infty$, the user does not click on any item. Since the user clicks on the first attractive item in the list, we can determine the observed weights of all recommended items at time $t$ from $\rnd{C}_t$. In particular, note that:
\begin{align}
  \rnd{w}_t(\rnd{a}^t_k) = \I{\rnd{C}_t = k} \quad k = 1, \dots, \min \set{\rnd{C}_t, K}\,.
  \label{eq:click}
\end{align}
We say that item $e$ is \emph{observed} at time $t$ if $e = \rnd{a}^t_k$ for some $1 \leq k \leq \min \set{\rnd{C}_t, K}$.

Recall that the weight of any item at time $t$ is drawn independently of the weights of the other items at that, or any other, time. This assumption has profound consequences. In particular, under this assumption, the expected reward for list $A \in \Pi_K(E)$, the probability that at least one item in $A$ is attractive, can be expressed as $\EE{f(A, \rnd{w})} = f(A, \bar{w})$, and depends only on the attraction probabilities of individual items in $A$.

The agent's policy is evaluated by its \emph{expected cumulative regret}:
\begin{align*}
  R(n) = \EE{\sum_{t = 1}^n R(\rnd{A}_t, \rnd{w}_t)}\,,
\end{align*}
where $R(\rnd{A}_t, \rnd{w}_t) = f(A^\ast, \rnd{w}_t) - f(\rnd{A}_t, \rnd{w}_t)$ is the \emph{instantaneous stochastic regret} of the agent at time $t$ and:
\begin{align*}
  A^\ast = \argmax_{A \in \Pi_K(E)} f(A, \bar{w})
\end{align*}
is the \emph{optimal list} of items, the list that maximized the reward at any time $t$. Since $f$ is invariant to the permutation of $A$, there exist at least $K!$ optimal lists. For simplicity of exposition, we assume that the optimal solution, as a set, is unique.

\subsubsection{Previous work}
todoc[inline]{Here we should summarize here what is known, the previous algorithms. Include the previous results.}

The model differs from the cascading bandit model of \citet{kveton15cascade} only in the assumption that for all items $e\in E$, the item's mean attractiveness can be written as the linear combination of the item's features:
$\bar{w}(e) = \psi(e)^\top \theta$.
Note that the choice $\psi_{e'}(e) = \I{e=e'}$ gives back the cascading bandit problem consider by \cite{kveton15cascade}, which we shall call the \emph{tabular} case.
Thus, the new model is at least as general as the previous one.
The motivation for the new model is to allow ``generalization'' across items and thus allow the learning agent to cope with problems when the number of items is possibly significantly larger than the number of timesteps.

For the tabular cascading bandit setting \citet{kveton15cascade} introduced two algorithms,  $\cascadeucb$ and $\cascadeklucb$
and proved upper bounds on their $n$-step regret.
todoc{Describe here briefly what the algorithms do.}
Here, we state the main results of this work.
To state these results, we need some extra definitions.

In particular, without loss of generality, we assume that the items in the ground set $E$
 are sorted in decreasing order of their attraction probabilities, $\bar{w}(1) \geq \ldots \geq \bar{w}(L)$. In this setting, the \emph{optimal solution} is $A^\ast = (1, \dots, K)$, and contains the first $K$ items in $E$. We say that item $e$ is \emph{optimal} if $1 \leq e \leq K$. Similarly, we say that item $e$ is \emph{suboptimal} if $K < e \leq L$. The \emph{gap} between the attraction probabilities of suboptimal item $e$ and optimal item $e^\ast$:
\begin{align}
  \Delta_{e, e^\ast} = \bar{w}(e^\ast) - \bar{w}(e)
  \label{eq:gap}
\end{align}
measures the hardness of discriminating the items. Whenever convenient, we view an ordered list of items as the set of items on that list.

The first main result of  \citet{kveton15cascade} is a bound on the regret of $\cascadeucb$:
\begin{theorem}
\label{thm:ucb1} The expected $n$-step regret of $\cascadeucb$ is bounded as:
\begin{align*}
  R(n) \leq
  \sum_{e = K + 1}^L \frac{12}{\Delta_{e, K}} \log n + \frac{\pi^2}{3} L\,.
\end{align*}
\end{theorem}
%todoc[inline]{Derive a bound on the worst-case regret of $\cascadeucb$ and include it here. How does it scale with $L$?}

As being assumed in the cascading bandit paper, the items in the ground set are sorted in decreasing order of their attraction probabilities, $\bar{w}(1) \geq \ldots \geq \bar{w}(L)$. In this setting, for $K < e \leq L$, $\Delta_{e,K} \geq \Delta_{K+1,K}$. The worst case of expected $n$-step regret is bounded by:
\begin{align*}
 R(n) \leq \sum_{e = K + 1}^L \frac{12}{\Delta_{K+1, K}} \log n + \frac{\pi^2}{3} L
 = (\frac{12}{\Delta_{K+1, K}} \log n + \frac{\pi^2}{3}) L - (\frac{12}{\Delta_{K+1, K}} \log n)(K+1) ,
\end{align*}
which scales linearly with $L$.


\citet{kveton15cascade}  also provided a result, proving a bound on the regret of $\cascadeklucb$, which we include next.
In this result, $\kl{a}{b}$ stands for the binary relative entropy, i.e., the Kullback Leibler (KL) divergence between the Bernoulli random variables with respective parameters $0\le a,b\le 1$.
\begin{theorem}
\label{thm:klucb} For any $\eps > 0$, the expected $n$-step regret of $\cascadeklucb$ is bounded as:
\begin{align*}
  R(n)
  & \leq \sum_{e = K + 1}^L
  \frac{(1 + \eps) \Delta_{e, K} (1 + \log(1 / \Delta_{e, K}))}{\kl{\bar{w}(e)}{\bar{w}(K)}} (\log n + 3 \log \log n) + C\,,
\end{align*}
where $C = K L \frac{C_2(\eps)}{n^{\beta(\eps)}} + 7 K \log \log n$, and the constants $C_2(\eps)$ and $\beta(\eps)$ are defined in \citet{garivier11klucb}.
\end{theorem}
%todoc[inline]{Derive a bound on the worst-case regret of $\cascadeklucb$ and include it here. How does it scale with $L$?}
As we can see here, for $\frac{(1 + \eps) \Delta_{e, K} (1 + \log(1 / \Delta_{e, K}))}{\kl{\bar{w}(e)}{\bar{w}(K)}} (\log n + 3 \log \log n)$,
the numerator is less than $1 + \eps$ since $\Delta_{e, K} (1 + \log(1 / \Delta_{e, K}))$ is less than 1 when $\Delta_{e, K}$ lies between 0 and 1,
while the denominator is always positive when $e > K$, and $\log n + 3 \log \log n$ is constant when $n$ is fixed. Therefore, this fraction can be bounded
by a finite constant number, and the summation scales linearly with $L$. Also, $C$ is linear with $L$ as its definition. Add these up we can say
the worst-case regret of $\cascadeklucb$ scales linearly with $L$

\subsection{Algorithms}
\label{sec:algorithms}
As can be seen, the regret for both algorithms scales linearly with $L$, the number of items.
%todoc{Prove it. Prove lower bound!}

We propose two algorithms for solving cascading bandits, $lincascadeucb$, building on $cascadeucb$ of
  \citet{kveton15cascade}, which itself builds on $ucb$ of \cite{auer02finitetime}.

\cref{alg:ucb} gives the pseudocode of $lincascadeucb$.
The algorithm is identical to $cascadeucb$ except that it estimates the
\emph{upper confidence bound (UCB)} $\rnd{U}_t(e)$ on the attraction probability of item $e$ at time $t$
in a way that exploits the linearity assumption.
In any case, given the estimates $\rnd{U}_t(e)$,
$lincascadeucb$ recommends a list of $K$ items with largest UCBs:
\begin{align}
  \rnd{A}_t = \argmax_{A \in \Pi_K(E)} f(A, \rnd{U}_t)\,.
\end{align}
Note that $\rnd{A}_t$ is determined only up to a permutation of the items in it. The payoff is not affected by this ordering. But the observations are. For now, we leave the order of items unspecified and return to it later in our discussions. After the user provides feedback $\rnd{C}_t$, the algorithms update the estimate of $\theta$
based on \eqref{eq:click}, for all $e = \rnd{a}^t_k$ where $k \leq \rnd{C}_t$.
In particular, the estimate computed is
\begin{align}
\label{eq:ridge}
\hat{\rnd{\theta}}_t = \argmin_{\theta\in \mathbb{R}^d}
\lambda \norm{\theta}^2 +
 \sum_{s=1}^{t-1} \sum_{k=1}^{K\wedge \rnd{C}_t} ( \I{\rnd{C}_t = k} - (\theta,\psi(\rnd{a}^t_k)))^2 \,,
\end{align}
where $a\wedge b \doteq \min(a,b)$, $\lambda>0$ is a tuning parameter and $\norm{\cdot}$ denotes the $2$-norm.
Note that $\hat{\theta}_t$ satisfies
\begin{align*}
  \hat{\rnd{\theta}}_t = \bar{V}_t^{-1} \sum_{s=1}^{t-1} \sum_{k=1}^{K \wedge C_t}
  \I{C_t=k} \psi(\rnd{a}^t_k)\,,
\end{align*}
where
\[
\bar{V}_t = \lambda I + \sum_{s=1}^{t-1} \sum_{k=1}^{K \wedge C_t}
\psi(\rnd{a}^t_k) \psi(\rnd{a}^t_k)^\top\,.
\]
Note that $\hat{\theta}_t$ can also be computed incrementally, based on the matrix inversion lemma.

todoc{Add details. Other computational considerations can be discussed later, i.e., $\hat{\theta}_t$ does need to be recomputed too frequently. See Yasin's thesis.}

The justification of the above estimation procedure is as follows:
For any given time step, $\I{C_t=1} = \rnd{w}_t(\rnd{a}^t_1)$.
Thus,
\begin{align*}
\EE{ \I{C_t=1} | \rnd{a}^t }
 =  \EE{ \rnd{w}_t(\rnd{a}^t_1) | \rnd{a}^t }
 = \bar{w}(\rnd{a}^t_1) = \psi(\rnd{a}^t_1)^\top \theta\,.
\end{align*}
Similarly,
\begin{align*}
\EE{ \I{C_t=2} | \rnd{a}^t, \rnd{w}_t(\rnd{a}^t_1 ) = 0 }
=  \EE{ \rnd{w}_t(\rnd{a}^t_2) | \rnd{a}^t, \rnd{w}_t(\rnd{a}^t_1 ) = 0  }
\end{align*}
\begin{align*}
\stackrel{\mathrm{\footnotesize (a)}}{=}
	  \EE{ \rnd{w}_t(\rnd{a}^t_2) | \rnd{a}^t }
\stackrel{\mathrm{\footnotesize (b)}}{=}
	\bar{w}(\rnd{a}^t_2) = \psi(\rnd{a}^t_2)^\top \theta\,.
\end{align*}

$\EE{ \rnd{w}_t(\rnd{a}^t_2) | \rnd{w}_t(\rnd{a}^t_1 ) = 1  } = 0$%\EE{ \rnd{w}_t(\rnd{a}^t_2)}$

Here, (a) holds because $\rnd{w}_t(\rnd{a}_2^t)$ is independent of $\rnd{w}_t(\rnd{a}_1^t)$

%todoc{Huanrui: As an exercise, prove this formally and send me the proof, or include it in an appendix here.}

As the assumption we made before, the weights, $\rnd{w}_t$, are distributed independently. Therefore, the independence of $\rnd{w}_t(\rnd{a}_1^t)$ and $\rnd{w}_t(\rnd{a}_2^t)$ will hold if $\rnd{a}^t$ is independent with $\rnd{w}_t$.

Let $\rnd{F}_{t-1}$ be the sigma algebra generated by $rnd{w}_s$ for $s = 1, \dots, t-1$. As $\rnd{a}^t$ can be computed via a deterministic function from the weights $\rnd{w}_1, \dots, \rnd{w}_{t-1}$, $\rnd{a}^t$ is $\rnd{F}_{t-1}$-measurable, $\EE{\rnd{w}_t | \rnd{a}^t , \rnd{F}_{t-1}} = \EE{\rnd{w}_t | \rnd{F}_{t-1}}$. As the assumption of the independence of sequence $(\rnd{w}_s)_s$ goes, it holds that $\rnd{w}_t$ is independent with $\rnd{F}_{t-1}$, therefore it holds that $\EE{\rnd{w}_t | \rnd{F}_{t-1}} = \EE{\rnd{w}_t}$.

Using the tower-rule, $\EE{\rnd{w}_t | \rnd{a}^t}=\EE{\EE{\rnd{w}_t | \rnd{a}^t , \rnd{F}_{t-1}} | \rnd{a}^t}=\EE{\rnd{w}_t}$. As the definition of independence goes, we can say $\rnd{a}^t$ is independent with $\rnd{w}_t$, there holds the independence of $\rnd{w}_t(\rnd{a}_1^t)$ and $\rnd{w}_t(\rnd{a}_2^t)$.

(which holds because $\rnd{a}^t$ and $\rnd{w}_t$ are independent of each other
and because the components of $\rnd{w}_t$ are independent of each other),
hence we can drop the condition on $\rnd{w}_t(\rnd{a}_1^t)$ without changing the expectation.
The equality marked as (b) holds because of our assumption that $\rnd{w}_t(e)$ is a Bernoulli with mean $\bar{w}(e)$ for any $e\in E$. Now, it should be clear that the same reasoning shows that for any $1\le k \le K$,
\begin{align*}
\EE{ \I{C_t=k} | \rnd{a}^t, \rnd{w}_t(\rnd{a}^t_1 ) = 0, \dots,  \rnd{w}_t(\rnd{a}^t_{k-1} ) = 0 }
= \psi(\rnd{a}^t_k)^\top \theta\,,
\end{align*}
showing that we can indeed view the data
$( (\psi(\rnd{a}^s_k), \I{C_s=k} ) )_{1\le s\le t, 1\le k \le K \wedge C_t }$ as being generated from a linear model.

%\begin{algorithm}[t]
%  \caption{UCB-like algorithm for linear cascading bandits.}
%  \label{alg:ucb}
%  \begin{algorithmic}
%    \STATE // Initialization
%    \STATE Observe $\rnd{X}_0 \sim P$
%    \STATE $\rnd{\theta}_0 \gets [\psi_{1:L} \psi_{1:L}^\top]^{-1} \psi_{1:L} \rnd{X}_0$
%    \STATE $\hat{\rnd{\theta}}_1 \gets \rnd{\theta}_0$
%    \STATE
%    \FORALL{$t = 1, \dots, n$}
%      \STATE Compute UCBs $\rnd{U}_t(e)$ %%(\cref{sec:algorithms})
%      \STATE
%      \STATE // Recommend a list of $K$ items and get feedback
%      \STATE Let $\rnd{a}^t_1, \dots, \rnd{a}^t_K$ be $K$ items with largest UCBs
%      \STATE $\rnd{A}_t \gets (\rnd{a}^t_1, \dots, \rnd{a}^t_K)$
%      \STATE Observe click $\rnd{C}_t \in \set{1, \dots, K, \infty}$
%      \STATE
%      \STATE // Update statistics
%      \STATE Define $\rnd{X}_t \in \set{0,1}^{\min \set{\rnd{C}_t, K}}$ as a binary vector.
%      \FORALL{$k = 1, \dots, \min \set{\rnd{C}_t, K}$}
%        \STATE $\rnd{X}_t(k) \gets \I{\rnd{C}_t = k}$
%      \ENDFOR
%      \STATE $\rnd{\theta}_t \gets [\psi_{\rnd{a}^t_1:\rnd{a}^t_k} \psi_{\rnd{a}^t_1:\rnd{a}^t_k}^\top]^{-1} \psi_{\rnd{a}^t_1:\rnd{a}^t_k} \rnd{X}_t$
%      \STATE $\displaystyle \hat{\rnd{\theta}}_t \gets \frac{(t - 1) \hat{\rnd{\theta}}_{t - 1} + \rnd{\theta}_t}{t}$
%    \ENDFOR
%  \end{algorithmic}
%\end{algorithm}
%
The UCBs are computed as follows. In $\cascadeucb$, the UCB on the attraction probability of item $e$ at time $t$ is:
\begin{align*}
  \rnd{U}_t(e) = \psi(e)^\top \hat{\rnd{\theta}}_t + \beta_t \norm{\psi(e) }_{\bar{V}_t^{-1}}\,,
\end{align*}
where $\norm{\cdot}_Q$ for $Q$ positive definite is defined by
$\norm{x}^2_Q  = x^\top Q x$ and
\[
\beta_t = \sqrt{2 \log\left(\frac{\mathrm{det}(\bar{V}_t/\lambda)}{\delta_t}\right)} + \lambda^{1/2} S\,,
\]
and $\delta_t = \delta/(t(t+1))$ and $\delta>0$ is a parameter to be chosen later.

\subsection{Confidence interval}
%todo{Compute confidence interval that contains the true value with probability $1-\delta$ based on Yasin's thesis.}

In Yasin's linear Stochastic Bandits paper, the Theorem of Self-Normalized Bound for Vector-Valued Martingales is stated as follow:
\begin{theorem}[Self-Normalized Bound for Vector-Valued Martingales]
\label{theorem:martingale-bound}
Let $\{F_t\}_{t=0}^\infty$ be a filtration. Let $\{\eta_t\}_{t=1}^\infty$ be a real-valued stochastic process such that $\eta_t$
is $F_t$-measurable and $\eta_t$ is conditionally $R$-sub-Gaussian for some $R \ge 0$ i.e.
\begin{align*}
\forall \lambda \in \R , \EE{e^{\lambda \eta_t} | F_{t-1}} \le \exp( \frac{\lambda^2R^2}{2}) \; .
\end{align*}
Let $\{X_t\}_{t=1}^\infty$ be an $\mathbb{R}^d$-valued stochastic process such that $X_t$ is $F_{t-1}$-measurable.
Assume that $V$ is a $d \times d$ positive definite matrix. For any $t \ge 0$, define
\begin{align*}
\bar{V}_t &= V + \sum_{s=1}^t X_s X_s^\top
&
S_t &= \sum_{s=1}^t \eta_s X_s \; .
\end{align*}
Then, for any $\delta > 0$, with probability at least $1-\delta$, for all $t \ge 0$,
$$
\norm{S_t}_{\bar{V}_t^{-1}}^2
\le
2 R^2 \log( \frac{\det (\bar{V}_t)^{1/2} \det(V)^{-1/2}}{\delta} ) \; .
$$
\end{theorem}


Let $\hat{\theta}_t$ be the $\ell^2$-regularized least-squares estimate of $\theta_*$ with regularization parameter $\lambda > 0$
\begin{align*}
  \hat{\rnd{\theta}}_t = \bar{V}_t^{-1} \sum_{s=1}^{t-1} \sum_{k=1}^{K \wedge C_s}
  \I{C_s=k} \psi(\rnd{a}^s_k)\,,
\end{align*}
where
\[
\bar{V}_t = \lambda I + \sum_{s=1}^{t-1} \sum_{k=1}^{K \wedge C_s}
\psi(\rnd{a}^s_k) \psi(\rnd{a}^s_k)^\top\,.
\]

Here, we define the following notations:
$X^s_{1 : K \wedge C_s}$ is the matrix whose rows are
$\psi(\rnd{a}^s_1)^\top, \psi(\rnd{a}^s_2)^\top, \dots, \psi(\rnd{a}^s_{ K \wedge C_s})^\top \,$ and
$Y_s = (\I{C_s=1}, \dots, \I{C_s= K \wedge C_s})^\top\,$

Using the definition of matrix multiplication, the formula above can be rewrite in a matrix form:
\begin{align*}
  \hat{\rnd{\theta}}_t = \bar{V}_t^{-1} \sum_{s=1}^{t-1} {X^s_{1 : K \wedge C_s}}^\top Y_s \,,
\end{align*}
\begin{align*}
\bar{V}_t = \lambda I + \sum_{s=1}^{t-1} {X^s_{1 : K \wedge C_s}}^\top X^s_{1 : K \wedge C_s} \,.
\end{align*}

To avoid clutter, let $X_s = X^s_{1 : K \wedge C_s}\,.$

As we defined in our model, $\I{C_s=k}$ can be expressed in a form of a inner product plus a random noise.
\begin{align*}
  \I{C_s=k} = \psi(\rnd{a}^s_k)^\top \theta_* + \rnd{\eta}_k \,.
\end{align*}
Where $ \rnd{\eta}_s $ follows a Bernoulli distribution that
has a probability of $\psi(\rnd{a}^s_k)^\top \theta_*$ to be $1 - \psi(\rnd{a}^s_k)^\top \theta_*$
and has a probability of $1 - \psi(\rnd{a}^s_k)^\top \theta_*$ to be $-\psi(\rnd{a}^s_k)^\top \theta_*$.

In this way, we can rewrite the expression of $Y_s$ :
\begin{align*}
  Y_s = X_s \theta_* + \rnd{\eta}_s \,,
\end{align*}
where
\begin{align*}
  \rnd{\eta}_s = (\rnd{\eta}_1, \dots, \rnd{\eta}_{K \wedge C_s})^\top \,,
\end{align*}

The following theorem shows that $\theta_*$ lies with high probability in an
ellipsoid with center at $\hat{\theta}_t$.

\begin{theorem}[Confidence Ellipsoid]
\label{theorem:confidence-ellipsoid}
Assume the same as in Theorem~\ref{theorem:martingale-bound}, let $V = I \lambda$, $\lambda > 0$, define
$Y_s = X_s \theta_* + \rnd{\eta}_s$ and assume that $\|\theta_*\|_2 \le S$. Then, for any $\delta > 0$, with probability at least $1-\delta$,
for all $t \ge 0$, $\theta_*$ lies in the set
$$
C_t = \{ \theta \in \R^d ~:~ \norm{\hat{\theta}_t -  \theta}_{\bar{V}_{t}}
$$

$$
\le \keyboundappliedp{t} + \lambda^{1/2}\, S \} \; .
$$

All confidence intervals hold for all time steps with probability $1-\delta$.

\end{theorem}

%follow the proof step by step here
\begin{proof}
We will need the following lemma, which is proved in Yasin's paper:

\begin{lemma}[Determinant-Trace Inequality]
\label{lemma:determinant-trace-inequality}
Suppose $X_1, X_2, \dots, X_t \in \R^d$ and for any $1 \le s \le t$, $\|X_s\|_2 \le L$.
Let $\V_t = \lambda I + \sum_{s=1}^t X_s X_s^\top$ for some $\lambda > 0$.
Then,
$$
\det(\V_t) \le (\lambda + tL^2/d)^d \; .
$$
\end{lemma}

As we mentioned before,
\begin{align*}
\hat{\theta}_t
&= (\sum_{s=1}^{t-1}{\X^\top \X} + \lambda I )^{-1} \sum_{s=1}^{t-1}{[\X^\top (\X \theta_* + \ETA)]} \\
&= (\sum_{s=1}^{t-1}{\X^\top \X} + \lambda I )^{-1} \sum_{s=1}^{t-1}{\X^\top \ETA} \\
&+ (\sum_{s=1}^{t-1}{\X^\top \X} + \lambda I )^{-1} (\sum_{s=1}^{t-1}{\X^\top \X+\lambda I}) \theta_* - \lambda (\sum_{s=1}^{t-1}{\X^\top \X} + \lambda I )^{-1}  \theta_* \\
&= (\sum_{s=1}^{t-1}{\X^\top \X} + \lambda I )^{-1} \sum_{s=1}^{t-1}{\X^\top \ETA} + \theta_* - \lambda (\sum_{s=1}^{t-1}{\X^\top \X} + \lambda I )^{-1}  \theta_* \; ,
\end{align*}

we get
\begin{align*}
x^\top \hat{\theta}_t - x^\top \theta_*
&= x^\top (\sum_{s=1}^{t-1}{\X^\top \X} + \lambda I )^{-1} \sum_{s=1}^{t-1}{\X^\top \ETA} \\
&- \lambda x^\top (\sum_{s=1}^{t-1}{\X^\top \X} + \lambda I )^{-1}  \theta_* \\
&= \ipp{x}{\sum_{s=1}^{t-1}{\X^\top \ETA}}_{\V_t^{-1}} - \lambda \ip{x}{\theta_*}_{\V_t^{-1}},
\end{align*}
where $\V_t = \sum_{s=1}^{t-1}{\X^\top \X} + \lambda I $.
Note that $\V_t$ is positive definite (thanks to $\lambda>0$) and hence so is $\V_t^{-1}$, so the above inner product is well-defined.
Using the Cauchy-Schwarz inequality, we get
\begin{align*}
  \abs{x^\top \hat{\rnd{\theta}}_t - x^\top \theta_*}
  &\leq \norm{x}_{\bar{V}_t^{-1}} (\norm{\sum_{s=1}^{t-1}{X_s^\top \rnd{\eta}_s}}_{\bar{V}_t^{-1}} + \lambda \norm{\theta_*}_{V_t^{-1}}) \\
  &\leq \norm{x}_{\bar{V}_t^{-1}} (\norm{\sum_{s=1}^{t-1}{X_s^\top \rnd{\eta}_s}}_{\bar{V}_t^{-1}} + \lambda^{0.5} \norm{\theta_*}_2) \,.
\end{align*}
where we used that $\norm{\theta_*}_{\V_t^{-1}}^2 \le 1/\lambda_{\min}(\V_t) \norm{\theta_*}_2^2 \le 1/\lambda \norm{\theta_*}_2^2$.

By Theorem~\ref{theorem:martingale-bound} with $V=\lambda I$, for any $\delta > 0$, with probability at least $1-\delta$,
 \begin{align*}
  \forall t \geq 0, \norm{\sum_{s=1}^{t-1}{X_s^\top \rnd{\eta}_s}}_{\bar{V}_t^{-1}}
  \end{align*}

  \begin{align*}
  \leq R \sqrt{2 \log\left(\frac{\mathrm{det}(\bar{V}_t)^{1/2} \mathrm{det}(\lambda I)^{-1/2} }{\delta}\right)}\,
\end{align*}

Therefore, on the event where this inequality holds, one also has
\begin{align*}
  \forall t \ge 0, \forall x \in \R^d, \abs{x^\top \hat{\rnd{\theta}}_t - x^\top \theta_*}
  \end{align*}

  \begin{align*}
  \leq \norm{x}_{\bar{V}_t^{-1}}(R \sqrt{2 \log\left(\frac{\mathrm{det}(\bar{V}_t)^{1/2} \mathrm{det}(\lambda I)^{-1/2} }{\delta}\right)} + \lambda^{0.5} \norm{\theta_*}_2) \,
\end{align*}

Plugging in  $x = \V_{t}(\hat{\theta}_t - \theta_*)$ and using $\|\theta_*\|_2 \le S$, we get
\begin{equation}
\label{eq:cs1}
\norm{\hat{\theta}_t -  \theta_*}_{\V_{t}}^2
\le
\norm{V_{t}(\hat{\theta}_t - \theta_*)}_{\V_{t}^{-1}} \left( \keyboundappliedp{t} + \lambda^{1/2}\, S\right).
\end{equation}
Now, $\norm{V_{t}(\hat{\theta}_t - \theta_*)}_{\V_{t}^{-1}} = \norm{\hat{\theta}_t -  \theta_*}_{\V_{t}}$ and therefore
dividing both sides by $\norm{\hat{\theta}_t -  \theta_*}_{\V_{t}}$ gives
$$
\norm{\hat{\theta_t} -  \theta_*}_{\V_{t}} \le \keyboundappliedp{t} + \lambda^{1/2}\, S \; .
$$
In other words, $\theta_* \in C_t$.

Since the whole process of the proof does not require any restriction on time step $t$, it can be stated that
all confidence intervals hold for all time steps with probability $1-\delta$.
\end{proof}


