%!TEX root = Paper.tex

\section{Related Work}
\label{sec:related work}

\emph{Ranked bandits} are a popular approach in learning to rank \cite{radlinski08learning} and they are closely related to our work. The key characteristic of ranked bandits is that each position in the recommended list is an independent bandit problem, which is solved by some \emph{base bandit algorithm}. The solutions in ranked bandits are $(1 - 1 / e)$ approximate and the regret is $\Omega(K)$ \cite{radlinski08learning}, where $K$ is the number of recommended items. Cascading bandits can be viewed as a form of ranked bandits where each recommended item attracts the user independently. We propose novel algorithms for this setting that can learn the optimal solution and whose regret decreases with $K$. We compare one of our algorithms to ranked bandits in \cref{sec:experiments ranked bandits}.

Our learning problem is of a combinatorial nature, our objective is to learn $K$ most attractive items out of $L$. In this sense, our work is related to stochastic combinatorial bandits, which are often studied with linear rewards and semi-bandit feedback \cite{gai12combinatorial,kveton14matroid,kveton14learning,kveton15tight}. The key differences in our work are that the reward function is non-linear in unknown parameters; and that the feedback is less than semi-bandit, only a subset of the recommended items is observed.

Our reward function is non-linear in unknown parameters. These types of problems have been studied before in various contexts. \citet{filippi10parametric} proposed and analyzed a generalized linear bandit with bandit feedback. \citet{chen13combinatorial} studied a variant of stochastic combinatorial semi-bandits whose reward function is a known monotone function of a linear function in unknown parameters. \citet{le14sequential} studied a network optimization problem whose reward function is a non-linear function of observations.

\citet{bartok12adaptive} studied finite partial monitoring problems. This is a very general class of problems with finitely many actions, which are chosen by the learning agent; and finitely many outcomes, which are determined by the environment. The outcome is unobserved and must be inferred from the feedback of the environment. Cascading bandits can be viewed as finite partial monitoring problems where the actions are lists of $K$ items out of $L$ and the outcomes are the corners of a $L$-dimensional binary hypercube. \citet{bartok12adaptive} proposed an algorithm that can solve such problems. This algorithm is computationally inefficient in our problem because it needs to reason over all pairs of actions and stores vectors of length $2^L$. \citet{bartok12adaptive} also do not prove logarithmic distribution-dependent regret bounds as in our work.

\citet{agrawal89asymptotically} studied a partial monitoring problem with non-linear rewards. In this problem, the environment draws a state from a distribution that depends on the action of the learning agent and an unknown parameter. The form of this dependency is known. The state of the environment is observed and determines reward. The reward is a known function of the state and action. \citet{agrawal89asymptotically} also proposed an algorithm for their problem and proved a logarithmic distribution-dependent regret bound. Similarly to \citet{bartok12adaptive}, this algorithm is computationally inefficient in our setting.

\citet{lin14combinatorial} studied partial monitoring in combinatorial bandits. The setting of this work is different from ours. \citet{lin14combinatorial} assume that the feedback is a linear function of the weights of the items that is indexed by actions. Our feedback is a non-linear function of the weights of the items.

\citet{mannor11bandits} and \citet{caron12leveraging} studied an opposite setting to ours, where the learning agent observes a superset of chosen items. \citet{chen14combinatorial} studied this problem in stochastic combinatorial semi-bandits.
