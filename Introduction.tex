%!TEX root = Paper.tex

\section{Introduction}
\label{sec:introduction}

\todoc[inline]{I copied the intro etc and did not change anything. This will need to be rewritten. We cannot plagiarize ourselves.}
The \emph{cascade model} is a popular model of user behavior in web search \cite{craswell08experimental}. In this model, the user is recommended a list of $K$ items, such as web pages. The user \emph{examines} the recommended list from the first item to the last, and selects the first \emph{attractive} item. In web search, this is manifested as a click. The items before the first attractive item are \emph{not attractive}, because the user examines these items but does not click on them. The items after the first attractive item are \emph{unobserved}, because the user never examines these items. The optimal list, the list of $K$ items that maximizes the probability that the user finds an attractive item, are $K$ most attractive items. The cascade model is simple but effective in explaining the so-called position bias in historical click data \cite{craswell08experimental}. Therefore, it is a reasonable model of user behavior.

In this paper, we propose an online learning variant of the cascade model, which we refer to as \emph{cascading bandits}. In this model, the learning agent does not know the attraction probabilities of items. At time $t$, the agent recommends to the user a list of $K$ items out of $L$ items and then observes the index of the item that the user clicks. If the user clicks on an item, the agent receives a reward of one. The goal of the agent is to maximize its total reward, or equivalently to minimize its cumulative regret with respect to the list of $K$ most attractive items. Our learning problem can be viewed as a bandit problem where the reward of the agent is a part of its feedback. But the feedback is richer than the reward. Specifically, the agent knows that the items before the first attractive item are not attractive.

We make five contributions. First, we formulate a learning variant of the cascade model as a stochastic combinatorial partial monitoring problem. Second, we propose two algorithms for solving it, $\cascadeucb$ and $\cascadeklucb$. $\cascadeucb$ is motivated by $\combucb$, a computationally and sample efficient algorithm for stochastic combinatorial semi-bandits \cite{gai12combinatorial,kveton15tight}. $\cascadeklucb$ is motivated by $\klucb$  and we expect it to perform better when the attraction probabilities of items are low \cite{garivier11klucb}. This setting is common in the problems of our interest, such as web search. Third, we prove gap-dependent upper bounds on the regret of our algorithms. Fourth, we derive a lower bound on the regret in cascading bandits. This bound matches the upper bound of $\cascadeklucb$ up to a logarithmic factor. Finally, we experiment with our algorithms on several problems. They perform well even when our modeling assumptions are not satisfied.

Our paper is organized as follows. In \cref{sec:background}, we review the cascade model. In \cref{sec:cascading bandits}, we introduce our learning problem and propose two UCB-like algorithms for solving it. In \cref{sec:analysis}, we derive gap-dependent upper bounds on the regret of $\cascadeucb$ and $\cascadeklucb$. In addition, we prove a lower bound and discuss how it relates to our upper bounds. We experiment with our learning algorithms in \cref{sec:experiments}. In \cref{sec:related work}, we review related work. We conclude in \cref{sec:conclusions}.
